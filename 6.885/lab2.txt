ProtoBuffers:

    Queries:
    1. Counted the number of tweets for which the `is_deleted` field was a
non-falsy value.
    2. Counted the number of insert tweets for which the `reply_to` field was
non-empty.
    3. Created a hash map, mapping user ids to counts of the number of
occurences of that id. Sorted the resulting map by the count and took the top
five.
    4. Created a hash map, mapping a place's id to a count of the number of
times the place was seen. Sorted the resulting map by the count and took the
top five.

    Results:
    1. 1554
    2. 2531
    3. 1269521828, 392695315, 424808364, 1706901902, 1471774728
    4. 'T\xfcrkiye', 'Gambir', 'East Borneo', 'Mississippi', 'Nongsa'

SQLite3:

    Queries:
    1. SELECT count(*) FROM tweets WHERE is_delete = 1;
    2. SELECT count(*) FROM tweets WHERE reply_to IS NOT NULL AND reply_to IS NOT 0;
    3. SELECT uid FROM tweets GROUP BY uid ORDER BY count(*) DESC LIMIT 5;
    4. SELECT name FROM places GROUP BY id ORDER BY count(*) DESC LIMIT 5;

    Results:
    1. 1554
    2. 2531
    3. 320820996, 1106510150, 183997528, 118947331, 206109994
    4. Türkiye, Gambir, East Borneo, Nongsa, Malalayang

MongoDB:

    Queries:
    1. db.tweets.count({delete: {$exists: true}})
    2. db.tweets.count({in_reply_to_status_id: {$exists: true, $ne: null}})
    3. db.tweets.aggregate([
         { $group: { _id: "$user.id", number: { $sum: 1 }}},
         { $sort: { number: -1 }},
         { $limit : 5 }
       ])
    4. db.tweets.aggregate([
         { $group: { _id: "$place.name", number: { $sum: 1}}},
         { $sort: { number: -1 }},
         { $limit: 5 }
       ])
 
    Results:
    1. 1554
    2. 2531
    3. 1269521828, 392695315, 1706901902, 424808364, 1279674000
    4. Türkiye, Gambir, Mississippi, East Borneo, Malalayang

Reflection:

1. Read the schema and protocol buffer definition files. What are the main 
differences between the two? Are there any similarities?

        The SQL schema file builds its definitions off of the protocol buffer
    defintion. The main difference is that the protocol buffer definition comes
    straight from the `twitter.json` file, so the protocol buffer file must do a
    lot more data cleaning. A lot of logic in the protocol buffer definition is
    spent with if statements to check for the presence of a particular field. The
    schema file has much less conditional logic because it has already been taken
    care of in the protocol buffers. The schema also takes care of null values.

        The two files are fairly similar, however, in the structure of the data.
    The same types of data categories are available, such as an insert,
    coordinate, place, etc. These different delineations of data stay the same
    throughout both methods.

2. Describe one question that would be easier to answer with protocol buffers
than via a SQL query.

        It would be easier to have complicated logic in protocol buffers because
    they are directly implemented in the python programming language. This means
    that the complexity of handling protocol buffers can be abstracted away and
    modularized just like any other python code could be.

        For a concrete example, imagine that you had email data, where each
    incoming email had a pointer to the email previous to it in an email thread,
    or nothing if it was the first email. The task would be to get the first email
    in each thread, and give the number of emails in the thread originating from
    that first email.

        Doing something like this in SQL would require obscure functions (like
    WITH RECURSIVE). However, the python language is expressive enough that this
    wouldn't be too much of a challenge.

3. Describe one question that would be easier to answer with MongoDB than via a
SQL query.

4. Describe one question that would be easier to answer via a SQL query than
using MongoDB.

5. What fields in the original JSON structure would be difficult to convert to
relational database schemas?


6. In terms of lines of code, when did various approaches shine? Think about the
challenges of defining schemas, loading and storing the data, and running
queries.

        The protobuffers were terrible in terms of lines of code. They required
    quite a bit of configuration and coding before anything could work. 

        The next worst was the SQL database. Most of the code came from the schema
    definition. It took quite a bit of code to actual convert the JSON into a
    format that SQL could understand.

        The best approach was MongoDB, since importing the Twitter json file was a
    piece of cake. Writing the queries were relatively simple once I got used to
    the language. Unfortunately, since MongoDB doesn't have a schema, the
    complexity of the MongoDB code was greater than the complexity of the SQL
    code. There was definitely a trade off between organizing the data beforehand
    and performing analysis on the data as is.

7. What other metrics (e.g., time to implement, code redundancy, etc.) can we use
to compare these different approaches? Which system is better by those
measures?

8. How long did this lab take you? We want to make sure to target future labs to
not take too much of your time.

    This lab took about 4 hours.
